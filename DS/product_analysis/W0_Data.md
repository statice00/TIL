## 1일차 - 20230419  
필요없는 부분(다른 문서에서 정리할 내용(통계이론, ..)은 제외하고 정리


sql 사용 예정  
구글 빅쿼리 사용 예정 : 설치할 필요 없고, 어디서든 환경이 바뀌어도 사용이 가능하기 때문

서비스가 기업이 추구하는 가치를 잘 반영하고 있는가
그떄마다 필요한 본질은 비슷하다
데이터에 잘 접근해서 분석에 용이하게 만들고 인사이트를 뽑는것

가장 세밀한 단위까지 쪼갤 수 있어야 분석이 잘 될 수 있다고 생각한다.
crm은 회사가 어떻게 하느냐에 따라서 고객이 느끼는것이 다르기에 분석이 어렵다

면접에서 신뢰수준, p-value 등등 이론적인 부분에 대한 질문을 자주 한다  

자격증 자체는 어필은 되지만 분석을 잘할것이다라는 신뢰는 안된다.  
결국 포트폴리오인데 깃허브, 블로그가 아니어도 ppt어도 
문제제기를 어떻게 했고 구상을 어떻게 했으며 해결하기 위해 어떻게 했다라고 일목요연하게 정리를 잘한 사람이 신뢰가 간다.

- 포트폴리오.. 이러면 안된다
포트폴리오를 링크로 첨부 -> 링크 접근 불가인지 확인 필수.. 전달 측면에서 치명적인 마이너스  
분석을 굳이 안해도 알 수 있는 결과는..  

해당 프로젝트에서만 끝나는 결과가 아니라 이 결과와 결론을 통해서 어떤 관점으로 더 확장할 수 있는지 생각해봤다~ 이런 내용 추가하는것도 좋다  
너무 많이 분석이 이루어진 분석과 결과들.. 전형적인 내용만 적는게 아니라 어떻게 하면 한번이라도 더 활용할 수 있을까? 고민하고 내용 추가하는 것이 좋다.  
ex) 너무 많이 분석과 결과가 이루어진 타이타닉 분석 -> 연령대,나이별로 사망률 분석하고 결과 내는게 대부분인데 여기서 멈추지 말고 실제 영화에 대입해서 남주인공, 여주인공의 사망사실을 확인하고 데이터 분석 결과와 얼마나 잘맞는지 비교하는 등 결과를 좀 더 확장~

빅쿼리를 가지고 구글시트랑 연결할 수 있다. 링크로도 공유가능

디스코드 참고해서 빅쿼리 세팅 미리 하기

매주차 만족도 조사 진행 

---

### data literacy

data : 의사결정을 위해 수집하고 분석하는 도구  
결국 **해석**의 영역이다  

데이터를 업무에 어떻게 활용할 수 있을지
- 문제 정의 or가설 설정
    - 모니터링을 통해
    - 분석 결과를 통해
- 분석 or 검증
    - 분석가 고유의 영역
- 분석 결과 확인 -> 결론 도출(스토리텔링)  
공유 받는 입장에서 편하게 이해할 수 있도록 전달하는 것
    - 결과와 결론은 다르다.

데이터 자체가 목적이 되면 안된다. 인사이트를 얻기 위한 수단이어야 한다.

- 데이터 중심
    - 데이터 그 자체를 보는 것
    - 좋아보이는 워딩이지만, 데이터 리터러시 측면에서믄 **가장 먼저 버려야 할 것**

- **목적 중심**  
추구해야 할 것
    - 왜 어째서 저런 데이터가 나한테 들어왔는지, 의문을 가지는 것

-> 데이터 문해력이란, "데이터에서 무언가를 읽어내는 능력"이 아니라, 스스로 정답에 대해 고민하고 데이터를 활용하여 합리적으로 논할 수 있는 능력

---

### Dark Data

"데이터를 활용"한다는 것  
올바른 목적을 설정하고 그 목적에 따라 데이터를 활용해서, 결과를 결론으로 이끌어낸다는 본질적인 흐름을 중시하고, 과정 하나하나 밟아가는것

-> 활용한 데이터가 모든 것을 설명해 주지는 않는다.  
데이터를 통해 특정 결론이 나왔다면, 그것은 활용한 데이터에 국한된 이야기라는것  

수집된 데이터는 말 그대로 결과값일 뿐, 필요한 내용들을 모두 담고있지 않는 경우가 많다.  
전혀 알 수 없는 소비자의 마음, 우리가 몰랐던 Seasonality, 통제 밖의 기타 외생적인 변수들..  

우리가 모르는 이러한 요소들로 인하여 데이터를 오해석하게 되고,  
결과적으로는 잘못된 의사결정을 하게 될 가능성이 있다. 어쩌다 한번이 아니라 항상 존재 가능성이 있다.


### data로 일하기

데이터로 일한다면 아래 두 가지를 짚고 넘어가기
- 비즈니스 **도메인** 파악
- 일할때 활용할 **데이터** 그 자체에 대해서 알아둘 필요가 있다.  

의외로 후순위인 것
- 이 데이터를 어떻게 파고들 것인지? 분석을 어떻게 할 지 등등
- 이외에, 분석가의 영역에 속하는 것들

[데이터 x User]  
데이터 존재를 확인한다면 알아보아야 할 것
- 어느 타이밍에 생성되었는지  
    - e.g., 결제 데이터
        - create_at, 그리고 updated_at 의 차이 -> 결제를 시도했을 때 / 결제대기 or 결제성공 or 결제실패 / 환불 발생
        - status -> done / fail / cancelled / ready
- 내가 원하는 데이터가 있는 지
    - e.g., 차량 공유 서비스
        - 예약시간 / 예약이 발생한 시간 / 이용 가능 시간 / 실제 이용 시작 시간
    
[내가 원하는 데이터 : 지표화 가능성]
- 측정 가능한 것
    - 정량
        양적 기준 평가
    - examples
        값 : 결제액 / 매출액 / 판매량  
        기준 : YoY(Year On Year, 전년 동기 대비), WoW(Week On Week, 지난 주 대비)

- 측정 불가능한 것
    - 정성
        질적 기준 평가
    - examples
        행복  
        기준 : 맛있는 음식 먹을때
    -> 정성적인 기준을 수치화 할 때 ~

### 숫자로 표현하기 (a.k.a 통계)

수학이나 통계, 코딩은 어느 수준까지 공부하는 것이 좋은 것인가

대표값(평균 등) 을 사용하는 이유  
-> 집단( 여러 사람 혹은 여러 케이스, 연속적인 변화 등 각각의 요소들이 모여서 형성한 자료들)에 대해 이야기를 할 때 편해짐

평균은 난잡해 보이는 데이터를 보기 쉽게 바꿀 수 있는 장점이 있지만, 평균의 함정을 조심해야 한다.(평균 수심 1미터라면 최대수심이 몇미터인지 같이 적어주는게 좋다) 즉, 이상치(outlier)에 영향을 많이 받는다.

평균의 함정을 보정하기 위해 median, percentile, mode, min, max 등을 같이 사용

대표값을 어떻게 신뢰? -> 신뢰수준, 신뢰구간

상관관계와 인과관계  
결과에 영향을 미치는 근본적인 원인이 존재함  
인과관계에서는 도메인 지식이 중요한 경우가 많다

#### 기초 통계 이후
- 매출을 예측해 볼 수 있을까?  
- 결제가 발생할 회원을 미리 알 수 있을까? -> LTV 계산  
- 특성 


```
-> 통계폴더에서 개념정리로 정리
```

---

### 총정리


---

### Data 직군과 SQL

각 공고에서 사용하는 툴을 보면 어떤 유형의 분석가인지 알 수 있다.  
Airflow를 사용한다 -> 엔지니어쪽도 다룬다
Python 위주다 -> 시각화 등등 전형적인 분석가 위주다

공고에서 데이터 엔지니어링 환경이 어떻게 되어있는지 알려주지 않는 경우가 많다.

데이터 처리 -> 데이터 엔지니어  
모델링 -> 데이터 사이언티스트  
인사이트, 의사 결정 제공 -> 데이터 분석가

분석보다 더 중요한 **전달 능력과 도메인 지식**

#### Data 밀접 직군

**Poduct Manager**  
의사결정을 끊임없이 해야하는 포지션

- 목표
    - (신사업) 서비스/제품 기획 및 런칭
    - 기 출시된 서비스/제품 고도화:추가 기능 붙이기

- 주로 듣는 말
    - 그래서 유저에게 어떤 영향이 생길까?
    - 이거 타임라인이?

**Performance Marketer**

- 목표
    - 최적화된 디지털 광고 집행
    - ROAS(광고비 지출한것 대비 매출액) 500%(광고비 이외에도 지출이 있기에 이 수치는 보통 손해)

- 주로 듣는 말
    - 이번달 CAC, CPA(Cost Per), CPC(Cost Per Customer) 는 어떻게 되나?

- 요약
    - 언제, 어느 타이밍에 광고를 송출할 것인가
    - 별도의 영업직군이 있지 않고, 광고를 통해 타겟을 유입시킨다면 매우 중요한 역할

- data 관련성
    - 각 채널 및 캠페인 성과에 따라서 어떻게 하면 더 유입을 끌어올 수 있을 지
    - 유입된 회원의 퀄리티를 어떻게 확보할 지


### Data 전처리

데이터 분석 단계  
데이터 확보 -> 데이터 분석/학습 -> 분석/모델링 결과 공유  

보통 데이터 전처리에 리소스를 80% 이상 쏟는게 맞다.  
전처리를 하다 보면 어떻게 분석을 해야겠다~하는 인사이트를 얻기 마련  

전처리는 어떤 주제인지에 따라 달라지고, 회사 내에서 미리 정해둔 규칙이 있다면 그대로 따른다

1. 데이터 확보  
분석 주제에 맞는 데이터를 확보하는 것  
- 내부 데이터 : SQL 혹은 API
    - DB에 접속하여 SQL로 Query를 날린다.
    - 서버에 접속하여 Postman(tool)으로 API를 호출한다.  
        
- 외부 데이터 : Scraping 혹은 API
    - 웹사이트 스크래핑
    - 공공 or 유료 API 찾아서 필요한 데이터 호출(Request) 해온다.
        법적으로 문제될만한 스크래핑은 주의

2. 데이터 클렌징
확보한 데이터를 검증하고 Cleansing하는 것 **가장 많은 리소스 소요됨 주의**

- 중복 데이터
    - 같은 데이터가 N개(N>1)
    - 중복이지만 중복이 아닌 것으로 잡히는 경우도 있다.
    - 경우에 따라서, 중복 데이터가 당연히 발생할 수 있음
    - 예시
        여자상업고등학교 여상고 여자상업고 등등 수기로 입력해서 천차만별이지만 결국 같은 단어들  
        늘찬배송(정부가 미는 순우리말) = 퀵서비스

- 이상치(Outlier)
    - 정상적인 범위에서 벗어난 데이터
    - 하지만 정상적인 경우를 가정하고 분석하는 경우가 많음
    - 따라서 Outlier 기준을 정하고 분석 대상에서는 제외하는 것이 일반적
    - Outlier가 궁금하면 Outlier를 포함한 분석이 아닌 Outlier만 모아서 분석하는 경우도 있다

- 결측치
    - Null N/A NA Na 등으로 표기됨
    - 0과는 다른 의미
        - 0은 관측이 이루어졌고, 0이라는 결과를 얻었다는 의미
        - 결측은 관측이 이루어지지 않아 내용을 모른다는 의미
        - 특정 값으로 대신하는 경우도 있음
    - **관측되지 않았다는 것 자체가 중요한 의미일 경우도 있음**

#### data 전처리 리소스 소모 예방 방법?  
사람이 직접 입력하면 일단 입력하시는 분들 각자 편한대로 입력하기 때문에 전처리는 불가피  
-> 학교 이름 검색하여 클릭 입력 api로 해결가증(주소입력, )

3. 분석하기 좋은 형태



- data type 맞추기
utc 











